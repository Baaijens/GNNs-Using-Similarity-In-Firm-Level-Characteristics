{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oAr9GuunEma"
      },
      "outputs": [],
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2\n",
        "!pip install google-api-python-client\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "AJBYgj60pvEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "m8S9FDdkpz5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Panel Data"
      ],
      "metadata": {
        "id": "RGt4_qs8EC0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Min_Charecteristics_Tilburg_1963_ALLSAMPLE.csv\")"
      ],
      "metadata": {
        "id": "C2HBofADECMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize data And lag feutures"
      ],
      "metadata": {
        "id": "7rqRZxQEEtAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Mom1m'] = df['excess_return']\n",
        "df['Y_excess_return'] = df['excess_return']\n",
        "# Remove duplicates while keeping the last occurrence\n",
        "df = df.sort_values('date').drop_duplicates(subset=['date', 'PERMNO'], keep='last')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = df.sort_values(by=['PERMNO', 'date'])\n",
        "# Shift the excess return by one month within each firm\n",
        "\n",
        "# List of column names to lag\n",
        "factor_columns = [\n",
        "    \"Beta\", \"RoE\", \"InvestPPEInv\", \"ShareIss5Y\", \"Accruals\", \"dNoa\",\n",
        "    \"GP\", \"AssetGrowth\",  \"Investment\", \"market_cap_adjusted\", \"excess_return\",\n",
        "    \"BM\", \"CompEquIss\", \"OperProf\",  \"MaxRet\", \"IndMom\", \"DolVol\" ,\"Mom1m\" , \"Mom6m\", \"Mom12m\"\n",
        "]\n",
        "\n",
        "\n",
        "# Loop through each column in the list and shift them forward by one period within each group\n",
        "for column in factor_columns:\n",
        "    df[column] = df.groupby('PERMNO')[column].shift(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# List of column names to standardize\n",
        "factor_columns = [\n",
        "    \"Beta\", \"RoE\", \"InvestPPEInv\", \"ShareIss5Y\", \"Accruals\", \"dNoa\",\n",
        "    \"GP\", \"AssetGrowth\",  \"Investment\", \"market_cap_adjusted\",\n",
        "    \"BM\", \"CompEquIss\", \"OperProf\",  \"MaxRet\", \"IndMom\", \"DolVol\" ,\"Mom1m\" , \"Mom6m\", \"Mom12m\"\n",
        "]\n",
        "# Compute the mean and standard deviation for each factor column grouped by 'date'\n",
        "means = df.groupby('date')[factor_columns].transform('mean')\n",
        "stds = df.groupby('date')[factor_columns].transform('std')\n",
        "\n",
        "# Standardize the existing columns without creating new ones\n",
        "for column in factor_columns:\n",
        "    df[column] = (df[column] - means[column]) / stds[column]\n",
        "\n",
        "\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "2cfOXYAbExcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLS"
      ],
      "metadata": {
        "id": "PQ_dMAm6ogC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Initialize a DataFrame to store all results\n",
        "all_results_df = pd.DataFrame()\n",
        "\n",
        "# Factor columns specified\n",
        "factor_columns = [\n",
        "    \"Beta\", \"RoE\", \"InvestPPEInv\", \"ShareIss5Y\", \"Accruals\", \"dNoa\",\n",
        "    \"GP\", \"AssetGrowth\", \"Investment\", \"market_cap_adjusted\",\n",
        "    \"BM\", \"CompEquIss\", \"OperProf\", \"MaxRet\", \"IndMom\", \"DolVol\", \"Mom1m\", \"Mom6m\", \"Mom12m\"\n",
        "]\n",
        "\n",
        "\n",
        "for test_year in range(1999, 2023):\n",
        "    # Update year ranges for training, validation, and test\n",
        "    train_years = (1969, 1986 + test_year - 1999)\n",
        "    val_years = (1987 + test_year - 1999, 1998 + test_year - 1999)\n",
        "    test_years = (test_year, test_year)\n",
        "\n",
        "    # Use masks for splitting the data\n",
        "    train_mask = df['date'].dt.year.isin(range(train_years[0], train_years[1] + 1))\n",
        "    val_mask = df['date'].dt.year.isin(range(val_years[0], val_years[1] + 1))\n",
        "    test_mask = df['date'].dt.year.isin(range(test_years[0], test_years[1] + 1))\n",
        "\n",
        "    # Split the data\n",
        "    train_df = df[train_mask]\n",
        "    val_df = df[val_mask]\n",
        "    test_df = df[test_mask]\n",
        "\n",
        "    # Debugging output\n",
        "    print(f\"Testing year: {test_year}\")\n",
        "    print(f\"Train range: {train_years}, Entries: {len(train_df)}\")\n",
        "    print(f\"Val range: {val_years}, Entries: {len(val_df)}\")\n",
        "    print(f\"Test range: {test_years}, Entries: {len(test_df)}\")\n",
        "\n",
        "    # Check if the test_df is empty\n",
        "    if test_df.empty:\n",
        "        print(f\"No data available for testing in year {test_year}. Skipping this year.\")\n",
        "        continue\n",
        "\n",
        "    # Combine training and validation sets\n",
        "    combined_train_val_df = pd.concat([train_df, val_df])\n",
        "\n",
        "    # Prepare data\n",
        "    X_combined_train_val = combined_train_val_df[factor_columns]\n",
        "    X_combined_train_val = sm.add_constant(X_combined_train_val)\n",
        "    y_combined_train_val = combined_train_val_df['Y_excess_return']\n",
        "\n",
        "    # Fit the OLS model\n",
        "    retrained_model = sm.OLS(y_combined_train_val, X_combined_train_val).fit()\n",
        "\n",
        "    # Prepare test data\n",
        "    X_test = test_df[factor_columns]\n",
        "    X_test = sm.add_constant(X_test)\n",
        "    y_test = test_df['Y_excess_return']\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred_test = retrained_model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "    # Collect results for this iteration\n",
        "    results_df = pd.DataFrame({\n",
        "        'PERMNO': test_df['PERMNO'],\n",
        "        'Date': test_df['date'],\n",
        "        'True_Y_excess_return': y_test,\n",
        "        'Predicted_Y_excess_return': y_pred_test,\n",
        "        'Test_MSE': test_mse\n",
        "    })\n",
        "\n",
        "    # Append results of this year to the all_results_df\n",
        "    all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)\n",
        "\n",
        "# Save the aggregated results to a single CSV file\n",
        "all_results_df.to_csv('Thesis Tilburg GNN/ResultsDF/OLS_all_years_SP.csv', index=False)\n",
        "\n",
        "print(\"Analysis complete. Results saved.\")\n"
      ],
      "metadata": {
        "id": "Zgn_uIHDnur-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RF"
      ],
      "metadata": {
        "id": "3gSvBygko7eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Initialize a DataFrame to store all results\n",
        "all_results_df = pd.DataFrame()\n",
        "\n",
        "# Factor columns specified\n",
        "factor_columns = [\n",
        "    \"Beta\", \"RoE\", \"InvestPPEInv\", \"ShareIss5Y\", \"Accruals\", \"dNoa\",\n",
        "    \"GP\", \"AssetGrowth\", \"Investment\", \"market_cap_adjusted\",\n",
        "    \"BM\", \"CompEquIss\", \"OperProf\", \"MaxRet\", \"IndMom\", \"DolVol\", \"Mom1m\", \"Mom6m\", \"Mom12m\"\n",
        "]\n",
        "\n",
        "# Configuration for the RandomForestRegressor\n",
        "n_estimators = 500  # Fixed number of trees\n",
        "max_depth_options = [1, 2, 3, 4, 5, 6]  # Range of tree depths\n",
        "max_features_options = [3, 5, 10, 20]  # Features considered at each split\n",
        "\n",
        "# Loop over each test year from 1999 to 2022\n",
        "for test_year in range(1999, 2023):\n",
        "    train_years = (1969, 1986 + test_year - 1999)\n",
        "    val_years = (1987 + test_year - 1999, 1998 + test_year - 1999)\n",
        "    test_years = (test_year, test_year)\n",
        "\n",
        "    # Create masks for splitting the data based on the specified years.\n",
        "    train_mask = df['date'].dt.year.between(train_years[0], train_years[1])\n",
        "    val_mask = df['date'].dt.year.between(val_years[0], val_years[1])\n",
        "    test_mask = df['date'].dt.year.between(test_years[0], test_years[1])\n",
        "\n",
        "    # Split the data into training, validation, and test sets.\n",
        "    train_df = df[train_mask]\n",
        "    val_df = df[val_mask]\n",
        "    test_df = df[test_mask]\n",
        "\n",
        "    if train_df.empty or val_df.empty or test_df.empty:\n",
        "        print(f\"Skipping year {test_year}: Missing data in one of the splits.\")\n",
        "        continue\n",
        "\n",
        "    # Prepare the training and validation data sets\n",
        "    X_train = train_df[factor_columns]\n",
        "    y_train = train_df['Y_excess_return']\n",
        "    X_val = val_df[factor_columns]\n",
        "    y_val = val_df['Y_excess_return']\n",
        "\n",
        "    # Hyperparameter tuning and model evaluation\n",
        "    best_val_score = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    for max_depth in max_depth_options:\n",
        "        for max_features in max_features_options:\n",
        "            model = RandomForestRegressor(\n",
        "                n_estimators=n_estimators,\n",
        "                max_depth=max_depth,\n",
        "                max_features=max_features,\n",
        "                random_state=42\n",
        "            )\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred_val = model.predict(X_val)\n",
        "            val_score = mean_squared_error(y_val, y_pred_val)\n",
        "\n",
        "            if val_score < best_val_score:\n",
        "                best_val_score = val_score\n",
        "                best_params = {\n",
        "                    'max_depth': max_depth,\n",
        "                    'max_features': max_features\n",
        "                }\n",
        "\n",
        "    # Retrain model with the best parameters on combined dataset\n",
        "    combined_train_val_df = pd.concat([train_df, val_df])\n",
        "    X_combined_train_val = combined_train_val_df[factor_columns]\n",
        "    y_combined_train_val = combined_train_val_df['Y_excess_return']\n",
        "    retrained_model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        **best_params,\n",
        "        random_state=42\n",
        "    )\n",
        "    retrained_model.fit(X_combined_train_val, y_combined_train_val)\n",
        "\n",
        "    # Predict on the test set\n",
        "    X_test = test_df[factor_columns]\n",
        "    y_test = test_df['Y_excess_return']\n",
        "    y_pred_test = retrained_model.predict(X_test)\n",
        "\n",
        "    # Calculate the test MSE for evaluation\n",
        "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "    # Collect results for this iteration\n",
        "    results_df = pd.DataFrame({\n",
        "        'PERMNO': test_df['PERMNO'],\n",
        "        'Date': test_df['date'],\n",
        "        'True_Y_excess_return': y_test,\n",
        "        'Predicted_Y_excess_return': y_pred_test,\n",
        "        'Test_MSE': test_mse\n",
        "    })\n",
        "\n",
        "    # Append results of this year to the all_results_df\n",
        "    all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)\n",
        "\n",
        "    # Print an update after processing each year\n",
        "    print(f\"Completed analysis for year {test_year}. Test MSE: {test_mse}\")\n",
        "\n",
        "# Save the aggregated results to a single CSV file\n",
        "all_results_df.to_csv('Thesis Tilburg GNN/ResultsDF/RF_all_years_SP.csv', index=False)\n",
        "\n",
        "print(\"Analysis complete. Results saved.\")\n"
      ],
      "metadata": {
        "id": "vgs6ZWP9o7T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBOOST"
      ],
      "metadata": {
        "id": "sf76uyTco7BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')  # Ignore all warnings\n",
        "\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Initialize an DataFrame to store all results\n",
        "all_results_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "factor_columns = [\n",
        "    \"Beta\", \"RoE\", \"InvestPPEInv\", \"ShareIss5Y\", \"Accruals\", \"dNoa\",\n",
        "    \"GP\", \"AssetGrowth\", \"Investment\", \"market_cap_adjusted\",\n",
        "    \"BM\", \"CompEquIss\", \"OperProf\", \"MaxRet\", \"IndMom\", \"DolVol\", \"Mom1m\", \"Mom6m\", \"Mom12m\"\n",
        "]\n",
        "\n",
        "# Loop through each test year from 1999 to 2022\n",
        "for test_year in range(1999, 2023):\n",
        "\n",
        "    train_years = (1969, 1986 + test_year - 1999)\n",
        "    val_years = (1987 + test_year - 1999, 1998 + test_year - 1999)\n",
        "    test_years = (test_year, test_year)\n",
        "\n",
        "    # Createe masks for the training, validation, and test splits\n",
        "    train_mask = df['date'].dt.year.between(*train_years)\n",
        "    val_mask = df['date'].dt.year.between(*val_years)\n",
        "    test_mask = df['date'].dt.year.between(*test_years)\n",
        "\n",
        "    # Split the DataFrame into training, validation, and test sets\n",
        "    train_df, val_df, test_df = df[train_mask], df[val_mask], df[test_mask]\n",
        "\n",
        "    # Skip the year if any of the splits are empty\n",
        "    if train_df.empty or val_df.empty or test_df.empty:\n",
        "        print(f\"Skipping year {test_year}: Missing data in one of the splits.\")\n",
        "        continue\n",
        "\n",
        "    # Separate features and target variable for training and validation sets\n",
        "    X_train, y_train = train_df[factor_columns], train_df['Y_excess_return']\n",
        "    X_val, y_val = val_df[factor_columns], val_df['Y_excess_return']\n",
        "\n",
        "    best_val_score, best_params = float('inf'), None\n",
        "\n",
        "    # Hyperparameter tuning for XGBoost model\n",
        "    for learning_rate in [0.01, 0.1]:\n",
        "        for max_depth in [1, 2]:\n",
        "            # Initialize and train the model\n",
        "            model = xgb.XGBRegressor(\n",
        "                n_estimators=30, max_depth=max_depth, learning_rate=learning_rate,\n",
        "                objective='reg:squarederror', tree_method='gpu_hist', verbosity=0, eval_metric='rmse'\n",
        "            )\n",
        "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
        "\n",
        "            # Predict on validation set and calculate validation score\n",
        "            y_pred_val = model.predict(X_val, iteration_range=(0, model.best_iteration + 1))\n",
        "            val_score = mean_squared_error(y_val, y_pred_val)\n",
        "\n",
        "            # Update best parameters if current validation score is better\n",
        "            if val_score < best_val_score:\n",
        "                best_val_score = val_score\n",
        "                best_params = {\n",
        "                    'n_estimators': model.best_iteration + 1,\n",
        "                    'max_depth': max_depth,\n",
        "                    'learning_rate': learning_rate\n",
        "                }\n",
        "\n",
        "    print(f\"Year: {test_year}, Best validation MSE: {best_val_score}, Params: {best_params}\")\n",
        "\n",
        "    # Combine training and validation sets for final training\n",
        "    combined_train_val_df = pd.concat([train_df, val_df])\n",
        "    X_combined_train_val = combined_train_val_df[factor_columns]\n",
        "    y_combined_train_val = combined_train_val_df['Y_excess_return']\n",
        "\n",
        "    # Retrain the model on the combined training and validation set with the best parameters\n",
        "    retrained_model = xgb.XGBRegressor(\n",
        "        **best_params, objective='reg:squarederror', tree_method='gpu_hist', verbosity=0\n",
        "    )\n",
        "    retrained_model.fit(X_combined_train_val, y_combined_train_val)\n",
        "\n",
        "    # Predict on test set and calculate test MSE\n",
        "    X_test = test_df[factor_columns]\n",
        "    y_test = test_df['Y_excess_return']\n",
        "    y_pred_test = retrained_model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "    # Create a DataFrame to store results for the current year\n",
        "    results_df = pd.DataFrame({\n",
        "        'PERMNO': test_df['PERMNO'],\n",
        "        'Date': test_df['date'],\n",
        "        'True_Y_excess_return': y_test,\n",
        "        'Predicted_Y_excess_return': y_pred_test,\n",
        "        'Test_MSE': test_mse\n",
        "    })\n",
        "\n",
        "    # Append current year results to the all results DataFrame\n",
        "    all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "all_results_df.to_csv('Thesis Tilburg GNN/ResultsDF/XGB_all_years.csv', index=False)\n",
        "print(\"Analysis complete. Results saved.\")"
      ],
      "metadata": {
        "id": "TOIeSYPqo6u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN3"
      ],
      "metadata": {
        "id": "yOHqztWon7_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Initialize a DataFrame to store all results\n",
        "all_results_df = pd.DataFrame()\n",
        "\n",
        "# Factor columns specified\n",
        "factor_columns = [\n",
        "    \"Beta\", \"RoE\", \"InvestPPEInv\", \"ShareIss5Y\", \"Accruals\", \"dNoa\",\n",
        "    \"GP\", \"AssetGrowth\",  \"Investment\", \"market_cap_adjusted\",\n",
        "    \"BM\", \"CompEquIss\", \"OperProf\", \"MaxRet\", \"IndMom\", \"DolVol\", \"Mom1m\", \"Mom6m\", \"Mom12m\"\n",
        "]\n",
        "\n",
        "# Data preparation and model configuration\n",
        "def build_model(l1_penalty, learning_rate, input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(32, activation='relu', input_shape=(input_shape,), kernel_regularizer=regularizers.l1(l1_penalty)),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(8, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "for test_year in range(1999, 2023):\n",
        "    # Update year ranges for training, validation, and test\n",
        "    train_years = (1969, 1986 + test_year - 1999)\n",
        "    val_years = (1987 + test_year - 1999, 1998 + test_year - 1999)\n",
        "    test_years = (test_year, test_year)\n",
        "\n",
        "    # Create masks for splitting the data basedd on the specified years.\n",
        "    train_mask = df['date'].dt.year.isin(range(train_years[0], train_years[1] + 1))\n",
        "    val_mask = df['date'].dt.year.isin(range(val_years[0], val_years[1] + 1))\n",
        "    test_mask = df['date'].dt.year.isin(range(test_years[0], test_years[1] + 1))\n",
        "\n",
        "    # Split the data into training, validation, and test sets.\n",
        "    train_df = df[train_mask]\n",
        "    val_df = df[val_mask]\n",
        "    test_df = df[test_mask]\n",
        "\n",
        "    # Prepare data\n",
        "    X_train = train_df[factor_columns]\n",
        "    y_train = train_df['Y_excess_return']\n",
        "    X_val = val_df[factor_columns]\n",
        "    y_val = val_df['Y_excess_return']\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    # Hyperparameter tuning setup\n",
        "    l1_penalties = [1e-5, 1e-3]\n",
        "    learning_rates = [0.001, 0.01]\n",
        "    batch_size = 512\n",
        "    epochs = 200\n",
        "\n",
        "    best_val_mse = float('inf')\n",
        "    best_config = None\n",
        "    best_epochs = 0\n",
        "\n",
        "    for l1_penalty in l1_penalties:\n",
        "        for learning_rate in learning_rates:\n",
        "            model = build_model(l1_penalty, learning_rate, X_train_scaled.shape[1])\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "            history = model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val),\n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "            val_mse = min(history.history['val_loss'])\n",
        "            optimal_epochs = early_stopping.stopped_epoch - 5\n",
        "            if val_mse < best_val_mse:\n",
        "                best_val_mse = val_mse\n",
        "                best_config = {'l1_penalty': l1_penalty, 'learning_rate': learning_rate}\n",
        "                best_epochs = optimal_epochs if optimal_epochs > 0 else max(history.epoch)\n",
        "\n",
        "    # Train 10 models and make predictions\n",
        "    combined_train_val_df = pd.concat([train_df, val_df])\n",
        "    X_combined_train_val = combined_train_val_df[factor_columns]\n",
        "    y_combined_train_val = combined_train_val_df['Y_excess_return']\n",
        "    X_combined_train_val_scaled = scaler.fit_transform(X_combined_train_val)\n",
        "    X_test_scaled = scaler.transform(test_df[factor_columns])\n",
        "    y_test = test_df['Y_excess_return']\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(10):\n",
        "        model = build_model(best_config['l1_penalty'], best_config['learning_rate'], X_combined_train_val_scaled.shape[1])\n",
        "        model.fit(X_combined_train_val_scaled, y_combined_train_val, epochs=best_epochs, batch_size=512, verbose=0)\n",
        "        y_pred = model.predict(X_test_scaled).flatten()\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "    # Average predictions from all models\n",
        "    y_pred_ensemble = np.mean(np.column_stack(predictions), axis=1)\n",
        "\n",
        "    # Calculate the test MSE for evaluation\n",
        "    test_mse = mean_squared_error(y_test, y_pred_ensemble)\n",
        "\n",
        "    # Collect results for this iteration\n",
        "    results_df = pd.DataFrame({\n",
        "        'PERMNO': test_df['PERMNO'],\n",
        "        'Date': test_df['date'],\n",
        "        'True_Y_excess_return': y_test,\n",
        "        'Predicted_Y_excess_return': y_pred_ensemble,\n",
        "        'Test_MSE': test_mse\n",
        "    })\n",
        "\n",
        "    # Append results of this year to the all_results_df\n",
        "    all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)\n",
        "\n",
        "# Save the aggregated results to a single CSV file\n",
        "all_results_df.to_csv('Thesis Tilburg GNN/ResultsDF/NN_all_years.csv', index=False)\n",
        "\n",
        "print(\"Analysis complete. Results saved.\")\n"
      ],
      "metadata": {
        "id": "s1H6MRDhn9Oy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}