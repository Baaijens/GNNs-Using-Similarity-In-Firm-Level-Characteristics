{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Set up OS"
      ],
      "metadata": {
        "id": "c3PheMoSS3Rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbsD5VJkMWfk"
      },
      "outputs": [],
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2\n",
        "!pip install google-api-python-client\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "XSVRlOvSS51M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "7ubrMMSgYLn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Graph Data Per Panel Data Conversion Method"
      ],
      "metadata": {
        "id": "1MN1TzvxRlKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "euclidean"
      ],
      "metadata": {
        "id": "vw7Nno9ORwJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch_geometric.data import DataLoader, Data\n",
        "from glob import glob\n",
        "\n",
        "def extract_parameters_from_filename(filename):\n",
        "    \"\"\"Extract W, sigma, and theta parameters from the given filename.\"\"\"\n",
        "    parts = filename.replace('.pt', '').split('_')\n",
        "    W, sigma, theta = None, None, None\n",
        "    for part in parts:\n",
        "        if part.startswith('W'):\n",
        "            W = int(part[1:])\n",
        "        elif part.startswith('sigma'):\n",
        "            sigma = float(part[5:])\n",
        "        elif part.startswith('theta'):\n",
        "            theta = float(part[5:])\n",
        "    return W, sigma, theta\n",
        "\n",
        "def load_all_graphs(W, sigma, theta):\n",
        "    \"\"\"Load all graphs matching the given W, sigma, and theta parameters.\"\"\"\n",
        "    save_dir = f\"Pair(1963 New)_Min_FILTERED_W{W}_sigma{sigma}_theta{theta}\"\n",
        "    all_files = glob(os.path.join(save_dir, 'Pair(1963 New)_Min_FILTERED_*.pt'))\n",
        "    all_data = []\n",
        "\n",
        "    for file_path in all_files:\n",
        "        basename = os.path.basename(file_path)\n",
        "        W_file, sigma_file, theta_file = extract_parameters_from_filename(basename)\n",
        "        if W == W_file and sigma == sigma_file and theta == theta_file:\n",
        "            graph_data = torch.load(file_path)\n",
        "            all_data.append(graph_data)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Specify parameters\n",
        "W_target = 36\n",
        "sigma_target = 1\n",
        "theta_target = 0.00001\n",
        "\n",
        "# Load all matching data\n",
        "all_data = load_all_graphs(W=W_target, sigma=sigma_target, theta=theta_target)\n"
      ],
      "metadata": {
        "id": "GhgpAp1CRqhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine"
      ],
      "metadata": {
        "id": "75iDkQvKRy-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch_geometric.data import DataLoader, Data\n",
        "from glob import glob\n",
        "\n",
        "def extract_parameters_from_filename(filename):\n",
        "    \"\"\"Extract W and Percentile parameters from the given filename.\"\"\"\n",
        "    parts = filename.replace('.pt', '').split('_')\n",
        "    W, percentile = None, None\n",
        "    for part in parts:\n",
        "        if part.startswith('W'):\n",
        "            W = int(part[1:])\n",
        "        elif 'Percentile' in part:\n",
        "            start = part.find('Percentile') + len('Percentile')\n",
        "            end = part.find('_Abs') if '_Abs' in part else len(part)\n",
        "            percentile = int(part[start:end])\n",
        "    return W, percentile\n",
        "\n",
        "def load_all_graphs(W, percentile):\n",
        "    \"\"\"Load all graphs matching the given W and Percentile parameters, and print nodes and edges per graph.\"\"\"\n",
        "    save_dir = f\"Pair-cosine(1963bestSP500)_Min_FILTERED_W{W}_Percentile{percentile}\"\n",
        "    all_files = glob(os.path.join(save_dir, '*.pt'))\n",
        "    all_data = []\n",
        "\n",
        "    for file_path in all_files:\n",
        "        basename = os.path.basename(file_path)\n",
        "        W_file, percentile_file = extract_parameters_from_filename(basename)\n",
        "        if W == W_file and percentile == percentile_file:\n",
        "            graph_data = torch.load(file_path)\n",
        "            all_data.append(graph_data)\n",
        "            print(f\"Graph loaded from {file_path}: {graph_data.num_nodes} nodes, {graph_data.num_edges // 2} edges.\")\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Specify the window size and percentile parameters\n",
        "W_target = 36  # Update this as needed\n",
        "percentile_target = 5  # Update this as needed\n",
        "\n",
        "# Load all matching data\n",
        "all_data = load_all_graphs(W=W_target, percentile=percentile_target)\n",
        "\n",
        "# Optional: Load into a DataLoader if using batches\n",
        "# data_loader = DataLoader(all_data, batch_size=10)  # Adjust batch size as necessary\n"
      ],
      "metadata": {
        "id": "JsWLHdFYSRuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation"
      ],
      "metadata": {
        "id": "fZHo-NL2R4VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch_geometric.data import DataLoader, Data\n",
        "from glob import glob\n",
        "\n",
        "def extract_parameters_from_filename(filename):\n",
        "    \"\"\"Extract W and theta parameters from the given filename.\"\"\"\n",
        "    parts = filename.replace('.pt', '').split('_')\n",
        "    W, theta = None, None\n",
        "    for part in parts:\n",
        "        if part.startswith('W'):\n",
        "            W = int(part[1:])\n",
        "        elif part.startswith('theta'):\n",
        "            theta = float(part[5:])\n",
        "    return W, theta\n",
        "\n",
        "def load_all_graphs(W, theta):\n",
        "    \"\"\"Load all graphs matching the given W and theta parameters.\"\"\"\n",
        "    save_dir = f\"Corrsaved(absolute)_graphs_W{W}_theta{theta}\"\n",
        "    all_files = glob(os.path.join(save_dir, '*.pt'))\n",
        "    all_data = []\n",
        "\n",
        "    for file_path in all_files:\n",
        "        basename = os.path.basename(file_path)\n",
        "        W_file, theta_file = extract_parameters_from_filename(basename)\n",
        "        if W == W_file and theta == theta_file:\n",
        "            graph_data = torch.load(file_path)\n",
        "            all_data.append(graph_data)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Specify parameters\n",
        "W_target = 36\n",
        "theta_target = 0.00001\n",
        "\n",
        "# Load all matching data\n",
        "all_data = load_all_graphs(W=W_target, theta=theta_target)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9hw65ETPSPPP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAT Model"
      ],
      "metadata": {
        "id": "SLm49-DioWHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Splitting Function**"
      ],
      "metadata": {
        "id": "hxtONIr7VLQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function splits the dataset into training, validation, and test sets based on specified year ranges."
      ],
      "metadata": {
        "id": "Q3uDuG9eVRG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_by_year(data, train_years, val_years, test_years):\n",
        "    \"\"\"Split the provided data into training, validation, and test datasets based on year ranges.\"\"\"\n",
        "    train_dataset, val_dataset, test_dataset = [], [], []\n",
        "\n",
        "    for graph_data in data:\n",
        "        if graph_data.date.numel() > 0:\n",
        "            year = int(str(graph_data.date[0].item())[:4])\n",
        "            if train_years[0] <= year <= train_years[1]:\n",
        "                train_dataset.append(graph_data)\n",
        "            elif val_years[0] <= year <= val_years[1]:\n",
        "                val_dataset.append(graph_data)\n",
        "            elif test_years[0] <= year <= test_years[1]:\n",
        "                test_dataset.append(graph_data)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n"
      ],
      "metadata": {
        "id": "T0iSCY9RVNHd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom GAT Layer**\n"
      ],
      "metadata": {
        "id": "ts5CmwjUVpRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class defines a custom Graph Attention Network (GAT) layer:\n",
        "\n",
        "__init__: Initializes the layer with specified input and output channels, attention heads, and dropout rates.\n",
        "forward: Defines the forward pass where input features x are processed through dropout, GAT convolution, and another dropout."
      ],
      "metadata": {
        "id": "1fyFP9kjVpGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGATLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, heads=1, dropout_rate=0.5, attention_dropout_rate=0.5):\n",
        "        super(CustomGATLayer, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.attention_dropout = torch.nn.Dropout(attention_dropout_rate)\n",
        "        self.gat_conv = GATConv(in_channels, out_channels, heads=heads, concat=True, dropout=dropout_rate, add_self_loops=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.dropout(x)\n",
        "        x = self.gat_conv(x, edge_index)\n",
        "        x = self.attention_dropout(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Z1wwqDgZVQ2y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GAT Model**"
      ],
      "metadata": {
        "id": "Vc48kRJQWIIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class defines a full GAT model:\n",
        "\n",
        "__init__: Initializes the model with multiple GAT layers and a final regressor layer.\n",
        "forward: Processes the input data through the GAT layers and produces a final regression output."
      ],
      "metadata": {
        "id": "sbkmtk4LWIBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, base_units, num_layers, heads, dropout_rate=0.5, attention_dropout_rate=0.5, negative_slope=0.2):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            in_channels = num_node_features if i == 0 else base_units * heads\n",
        "            layer = CustomGATLayer(in_channels, base_units, heads, dropout_rate, attention_dropout_rate)\n",
        "            self.layers.append(layer)\n",
        "        self.regressor = torch.nn.Linear(base_units * heads, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x[:, :-1], data.edge_index\n",
        "        for layer in self.layers:\n",
        "            x = F.elu(layer(x, edge_index))\n",
        "        x = self.regressor(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "27dxp4MYWI-4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Function**"
      ],
      "metadata": {
        "id": "T0YR58CmWKcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function trains the model on the provided data loader:\n",
        "\n",
        "train: Puts the model in training mode.\n",
        "for data in loader: Iterates over batches of data to train the model.\n",
        "optimizer.zero_grad(): Resets gradients.\n",
        "out = model(data): Performs a forward pass.\n",
        "loss.backward(): Computes gradients.\n",
        "optimizer.step(): Updates model parameters."
      ],
      "metadata": {
        "id": "hoLDEvUxWKZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, l1_lambda, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        target = data.x[:, -1].view(-1, 1).to(device)\n",
        "        mse_loss = criterion(out, target)\n",
        "        l1_loss = sum(p.abs().sum() for p in model.parameters())\n",
        "        loss = mse_loss + l1_lambda * l1_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "Ey_q7unhWKOT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Function**"
      ],
      "metadata": {
        "id": "zqLt4hWZWKEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function validates the model:\n",
        "\n",
        "validate: Puts the model in evaluation mode.\n",
        "with torch.no_grad(): Disables gradient computation for validation.\n",
        "for data in loader: Iterates over batches of data to validate the model.\n"
      ],
      "metadata": {
        "id": "DMFKJ-dmWKA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            target = data.x[:, -1].view(-1, 1).to(device)\n",
        "            loss = criterion(out, target)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "F70mEDpnWJ5K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluating Across Years**"
      ],
      "metadata": {
        "id": "eHZIjegJWJu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function trains and evaluates the model across multiple years:\n",
        "\n",
        "for test_year in range(start_year, end_year + 1): Iterates over each test year.\n",
        "split_data_by_year: Splits the data for each test year.\n",
        "for params in itertools.product(*hyperparams.values()): Iterates over all hyperparameter combinations.\n",
        "train: Trains the model.\n",
        "validate: Validates the model.\n",
        "ensemble_predictions: Averages predictions over multiple runs."
      ],
      "metadata": {
        "id": "nelSJJ53WJog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_all_years(data, start_year, end_year, num_ensemble_runs=10):\n",
        "    results_df = pd.DataFrame()\n",
        "\n",
        "    for test_year in range(start_year, end_year + 1):\n",
        "        print(f\"\\nStarting evaluations for test year: {test_year}\")\n",
        "        train_years = (1969, 1986 + (test_year - 1999))\n",
        "        val_years = (1987 + (test_year - 1999), 1998 + (test_year - 1999))\n",
        "        test_years = (test_year, test_year)\n",
        "\n",
        "        train_dataset, val_dataset, test_dataset = split_data_by_year(data, train_years, val_years, test_years)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_params = {}\n",
        "        patience = 5\n",
        "        best_model = None\n",
        "\n",
        "        for params in itertools.product(*hyperparams.values()):\n",
        "            lr, num_layers, base_units, batch_size, dropout_rate, attention_dropout_rate, l1_lambda, heads, negative_slope = params\n",
        "            model = GATModel(num_node_features-1, base_units, num_layers, heads, dropout_rate, attention_dropout_rate, negative_slope).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            best_val_loss = float('inf')\n",
        "            epochs_no_improve = 0\n",
        "            for epoch in range(500):\n",
        "                train_loss = train(model, train_loader, optimizer, criterion, l1_lambda, device)\n",
        "                val_loss = validate(model, val_loader, criterion, device)\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_model = model\n",
        "                    epochs_no_improve = 0\n",
        "                    print(f\"Validation improvement at epoch {epoch+1}, loss: {val_loss:.4f}\")\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    print(f\"No improvement at epoch {epoch+1}, current validation loss: {val_loss:.4f}\")\n",
        "                    if epochs_no_improve >= patience:\n",
        "                        print(f\"Early stopping triggered after {epoch+1} epochs for test year {test_year}.\")\n",
        "                        break\n",
        "\n",
        "            if best_val_loss < best_loss:\n",
        "                best_loss = best_val_loss\n",
        "                best_params = dict(zip(hyperparams.keys(), params), epochs=epoch+1 )\n",
        "\n",
        "        print(f\"Best parameters for year {test_year}: {best_params}\")\n",
        "        print(f\"Best validation loss for year {test_year}: {best_loss:.4f}\")\n",
        "\n",
        "        ensemble_predictions = []\n",
        "        identifiers_list = []\n",
        "        true_values = []\n",
        "        for run in range(num_ensemble_runs):\n",
        "            full_train_data = train_dataset + val_dataset\n",
        "            full_train_loader = DataLoader(full_train_data, batch_size=best_params['batch_size'], shuffle=True)\n",
        "            model = GATModel(num_node_features-1, best_params['base_units'], best_params['num_layers'], best_params['heads'], best_params['dropout_rate'], best_params['attention_dropout_rate'], best_params['negative_slope']).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "            for epoch in range(best_params['epochs']):\n",
        "                train_loss = train(model, full_train_loader, optimizer, criterion, best_params['l1_lambda'], device)\n",
        "                print(f\"Retraining on full data at epoch {epoch+1}, loss: {train_loss:.4f} for ensemble run {run+1}\")\n",
        "\n",
        "            test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
        "            predictions, identifiers, truths = predict_with_truths(model, test_loader, device)\n",
        "            ensemble_predictions.append(predictions)\n",
        "            identifiers_list = identifiers\n",
        "            true_values.append(truths)\n",
        "            print(f\"Run {run+1}, Test Prediction Completed. Mean prediction: {sum(predictions)/len(predictions):.4f}\")\n",
        "\n",
        "        mean_predictions = [sum(preds) / num_ensemble_runs for preds in zip(*ensemble_predictions)]\n",
        "        mean_true_values = [sum(trues) / num_ensemble_runs for trues in zip(*true_values)]\n",
        "        year_results = pd.DataFrame({\n",
        "            'Permno': [id[0] for id in identifiers_list],\n",
        "            'Date': [id[1] for id in identifiers_list],\n",
        "            'Prediction': mean_predictions,\n",
        "            'True': mean_true_values,\n",
        "            'Test_Year': [test_year]*len(mean_predictions)\n",
        "        })\n",
        "        results_df = pd.concat([results_df, year_results], ignore_index=True)\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "Wv92kI4XWJdS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict with Truths Function**"
      ],
      "metadata": {
        "id": "j7-bDJ44VRyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function generates predictions and collects true values from the data:\n",
        "\n",
        "model.eval(): Sets the model to evaluation mode.\n",
        "with torch.no_grad(): Disables gradient computation.\n",
        "identifiers.extend(zip(permnos, dates)): Collects unique identifiers for each data point.\n"
      ],
      "metadata": {
        "id": "TLc2uT4RXUnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_truths(model, loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    identifiers = []\n",
        "    truths = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            output = model(batch)\n",
        "            predictions.extend(output.view(-1).tolist())\n",
        "            truths.extend(batch.x[:, -1].view(-1).tolist())\n",
        "\n",
        "            if hasattr(batch, 'permno') and hasattr(batch, 'date'):\n",
        "                permnos = batch.permno.tolist()\n",
        "                dates = batch.date.tolist()\n",
        "                identifiers.extend(zip(permnos, dates))\n",
        "            else:\n",
        "                raise AttributeError(\"Batch does not contain 'permno' or 'date' attributes. Please check your dataset structure.\")\n",
        "\n",
        "    return predictions, identifiers, truths\n"
      ],
      "metadata": {
        "id": "wUdBxk9NVQRo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Execution**"
      ],
      "metadata": {
        "id": "_iscUOIzX6Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This final section:\n",
        "\n",
        "train_and_evaluate_all_years: Trains and evaluates the model across the specified years.\n",
        "save_results: Saves the results to CSV files.\n",
        "results_df.to_csv: Saves the aggregated results to a final CSV file."
      ],
      "metadata": {
        "id": "USbgEeAzX6Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use device settings\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "num_node_features = 20\n",
        "hyperparams = {\n",
        "    'learning_rate': [0.001],\n",
        "    'num_layers': [2],\n",
        "    'base_units': [8,16,32],\n",
        "    'batch_size': [24],\n",
        "    'dropout_rate': [0.3],\n",
        "    'attention_dropout_rate': [0],\n",
        "    'l1_lambda': [ 0],\n",
        "    'heads': [ 6,8,10,12],\n",
        "    'negative_slope': [ 0.01]\n",
        "}\n",
        "\n",
        "results_df = train_and_evaluate_all_years(all_data, 1999, 2022)\n",
        "print(\"Aggregated results across all years\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#file_name = f'Thesis Tilburg GNN/ResultsDF/Results(SP)_GAT_1963_Corr(absolute)_Min_W{W_target}_Percentile{percentile_target}.csv' #changhe this per panel data conversion method\n",
        "#results_df.to_csv(file_name, index=False)\n",
        "\n",
        "\n",
        "file_name = f'Thesis Tilburg GNN/ResultsDF/Results_GAT_1963_Correlation(absolute)_Min_W{W_target}_theta{theta_target}.csv'\n",
        "\n",
        "results_df.to_csv(file_name, index=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Uw7z49frVSco"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}